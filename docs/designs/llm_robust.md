这个 LLM 鲁棒性改进（LLM Robustness Improvements）的提案已经全案实施并验证完成。以下是该提案的功能与实现方式的详细中文总结：

## 1. 核心目标
主要解决了 LLM 在处理大规模数据（如数小时的屏幕截图）时面临的网络超时、缺乏监控、上下文超限以及效率不均衡的问题。

## 2. 主要功能模块

### A. 观测性与指标收集 (Observability & Metrics)
* 功能：实时监控每一次 LLM 请求的状态，不再是“黑盒”运行。
* 实现：
    * 引入了 LLMMetricsCollector  单例类，记录 P50/P95/P99 响应延迟。
    * 错误分类：将错误自动归类为 timeout（超时）、rate_limit（频率限制）、auth（鉴权）、server_error（后端异常）等，便于精准排查。
    * 日志系统：在控制台输出结构化日志 [LLMMetrics]，包含 Token 消耗和分块信息。

### B. 流式响应支持 (Streaming Response)
* 功能：提高用户感知的响应速度，并解决反向代理（如 Nginx/Vercel）的长连接超时问题。
* 实现：
    * 升级 LLMProvider 接口，支持 AsyncGenerator 流式输出。
    * 空闲超时检测 (Idle Timeout)：如果流式传输中间中断超过 30 秒无新数据，自动触发超时保护并尝试回退/重试。

### C. 自动分块处理 (Context Chunking)
* 功能：防止一次性发送过多截图导致超出模型上下文窗口或触发 API 载荷限制（Payload too large）。
* 实现：
    * 在 LLMService 中实现分块逻辑，根据配置（默认每 15 张图）自动拆分大批次任务。
    * 智能合并：多轮请求后的 ActivityCard 和 Observation 会自动按时间戳重新排序并合并，确保用户看到的结果是连贯的。

### D. 自适应分块动态调整 (Adaptive Chunking) - 亮点功能
* 功能：根据网络环境和模型性能自动调整“每块截图数量”，在快网速下提速，在不稳定环境下保活。
* 实现：
    * 单张耗时计算：通过 secondsPerShot 评估当前性能。
    * 滞后机制 (Hysteresis)：为了防止频繁跳变，要求连续 3 次“慢速”才缩小分块，连续 3 次“快速”才增大分块。
    * 冷却时间 (Cooldown)：调整一次后，必须等待 5 次请求让状态稳定，再进行下一次评估。
    * 紧急 shrink：如果最近 5 次请求出现 2 次超时，立即强制大幅缩减分块大小（Emergency Shrink）。

## 3. 技术实现方案
### 1. 架构设计：
    * Metrics 层：独立于 Provider，通过单例管理内存中的统计数据，并提供 IPC 接口供渲染进程查询。
    * Service 层：作为“协调员”，负责分块逻辑、重试策略以及与 Metrics 交互获取最新的自适应块大小。
    * Provider 层：具体实现 OpenAI 或 Gemini 的调用逻辑，同时负责上报性能数据。

### 2. 配置驱动：
* 在 llm_config.json 中定义了完整的自适应参数（如 slowSecsPerShot, maxSize 等），用户或开发者可以根据实际模型微调平衡点。

### 3. 验证体系：
* 建立了 26 项测试用例，包括：
    * 单元测试：验证分块边界、流式异常处理、指标计算准确性。
    * 集成测试：模拟真实 Pipeline，验证在模拟的“慢速网络”下，系统是否真的会收缩分块大小并成功完成任务。

## 4. 交付成果
* 稳定性：大幅降低了长序列分析时的失败率。
* 反馈感：流式处理让 UI 能够更早展现分析进度。
* 可维护性：通过指标数据，开发者现在可以清晰看到哪个模型在什么时候变慢或报错。
该功能目前已通过全套回归测试（196 tests passed），不仅提升了现有功能的鲁棒性，也为未来支持多模态长文本输入打下了坚实基础。